{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4441d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "url = \"https://www.ncei.noaa.gov/data/oceans/argo/gadr/data/indian/2019/01/\"\n",
    "folder = \"./data\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Get list of .nc files\n",
    "r = requests.get(url)\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "files = [node.get(\"href\") for node in soup.find_all(\"a\") if node.get(\"href\").endswith(\".nc\")]\n",
    "\n",
    "def download_file(f):\n",
    "    r_file = requests.get(url + f)\n",
    "    with open(os.path.join(folder, f), \"wb\") as file:\n",
    "        file.write(r_file.content)\n",
    "    print(\"Downloaded:\", f)\n",
    "\n",
    "# Download in parallel using 5 threads\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    executor.map(download_file, files)\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e68b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "input_folder = r\"C:\\Users\\shubh\\Downloads\\Classroom\\Hackathons\\SIH 2025\\ARGO\\Test\\data\"\n",
    "output_folder = r\"C:\\Users\\shubh\\Downloads\\Classroom\\Hackathons\\SIH 2025\\ARGO\\Test\\processed\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_file = os.path.join(output_folder, \"argo_indian_2019_01.parquet\")\n",
    "\n",
    "def process_nc(file_path):\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    dfs = []\n",
    "\n",
    "    n_levels = ds.sizes['n_levels']\n",
    "    n_prof = ds.sizes['n_prof']\n",
    "\n",
    "    for p in range(n_prof):\n",
    "        # Get profile-level info\n",
    "        lat = float(ds['latitude'].values[p])\n",
    "        lon = float(ds['longitude'].values[p])\n",
    "        juld = pd.Timestamp(ds['juld'].values[p])\n",
    "        float_id = str(ds['platform_number'].values[p])\n",
    "        cycle = int(ds['cycle_number'].values[p])\n",
    "        \n",
    "        # Profile data per level\n",
    "        pres = ds['pres'].values[p, :].flatten()\n",
    "        temp = ds['temp'].values[p, :].flatten()\n",
    "        psal = ds['psal'].values[p, :].flatten()\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"float_id\": [float_id]*n_levels,\n",
    "            \"cycle\": [cycle]*n_levels,\n",
    "            \"latitude\": [lat]*n_levels,\n",
    "            \"longitude\": [lon]*n_levels,\n",
    "            \"time\": [juld]*n_levels,\n",
    "            \"pressure\": pres,\n",
    "            \"temperature\": temp,\n",
    "            \"salinity\": psal\n",
    "        })\n",
    "        dfs.append(df)\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Process all files\n",
    "# ----------------------------\n",
    "all_files = glob.glob(os.path.join(input_folder, \"*.nc\"))\n",
    "all_dfs = []\n",
    "\n",
    "print(f\"Found {len(all_files)} NetCDF files. Processing...\")\n",
    "\n",
    "for f in all_files:\n",
    "    try:\n",
    "        df = process_nc(f)\n",
    "        all_dfs.append(df)\n",
    "        print(f\"Processed: {os.path.basename(f)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {f}: {e}\")\n",
    "\n",
    "if all_dfs:\n",
    "    argo_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    argo_df.to_parquet(output_file, index=False)\n",
    "    print(f\"Done! Combined data saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"No data processed. Check your files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c0a8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      float_id  cycle  latitude  longitude                          time  \\\n",
      "0  b'1900975 '    339   -48.136    128.389 2019-01-06 19:10:56.002052864   \n",
      "1  b'1900975 '    339   -48.136    128.389 2019-01-06 19:10:56.002052864   \n",
      "2  b'1900975 '    339   -48.136    128.389 2019-01-06 19:10:56.002052864   \n",
      "3  b'1900975 '    339   -48.136    128.389 2019-01-06 19:10:56.002052864   \n",
      "4  b'1900975 '    339   -48.136    128.389 2019-01-06 19:10:56.002052864   \n",
      "\n",
      "   pressure  temperature   salinity  \n",
      "0       4.2        9.198  34.480000  \n",
      "1      10.2        9.192  34.478001  \n",
      "2      17.4        9.206  34.481998  \n",
      "3      24.6        9.211  34.483002  \n",
      "4      31.1        9.194  34.478001  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(r\"C:\\Users\\shubh\\Downloads\\Classroom\\Hackathons\\SIH 2025\\ARGO\\Test\\processed\\argo_indian_2019_01.parquet\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dabb490f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374813"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb23ef22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1374813 entries, 0 to 1374812\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count    Dtype         \n",
      "---  ------       --------------    -----         \n",
      " 0   float_id     1374813 non-null  object        \n",
      " 1   cycle        1374813 non-null  int64         \n",
      " 2   latitude     1374813 non-null  float64       \n",
      " 3   longitude    1374813 non-null  float64       \n",
      " 4   time         1374813 non-null  datetime64[ns]\n",
      " 5   pressure     1089903 non-null  float32       \n",
      " 6   temperature  1085000 non-null  float32       \n",
      " 7   salinity     1084049 non-null  float32       \n",
      "dtypes: datetime64[ns](1), float32(3), float64(2), int64(1), object(1)\n",
      "memory usage: 68.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e873ace6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cycle</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>pressure</th>\n",
       "      <th>temperature</th>\n",
       "      <th>salinity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.374813e+06</td>\n",
       "      <td>1.374813e+06</td>\n",
       "      <td>1.374813e+06</td>\n",
       "      <td>1374813</td>\n",
       "      <td>1.089903e+06</td>\n",
       "      <td>1.085000e+06</td>\n",
       "      <td>1.084049e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.164047e+02</td>\n",
       "      <td>-3.015141e+01</td>\n",
       "      <td>9.298543e+01</td>\n",
       "      <td>2019-01-16 12:07:59.426492928</td>\n",
       "      <td>8.110287e+02</td>\n",
       "      <td>8.489975e+00</td>\n",
       "      <td>3.459978e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-6.443400e+01</td>\n",
       "      <td>2.058160e+01</td>\n",
       "      <td>2019-01-01 00:41:32.640000</td>\n",
       "      <td>-3.171200e+03</td>\n",
       "      <td>-1.709000e+00</td>\n",
       "      <td>-4.010000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>-4.244269e+01</td>\n",
       "      <td>7.010800e+01</td>\n",
       "      <td>2019-01-08 22:04:32.999994368</td>\n",
       "      <td>2.400000e+02</td>\n",
       "      <td>3.298000e+00</td>\n",
       "      <td>3.457620e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.040000e+02</td>\n",
       "      <td>-3.184380e+01</td>\n",
       "      <td>9.536275e+01</td>\n",
       "      <td>2019-01-16 11:50:15.999989248</td>\n",
       "      <td>7.240000e+02</td>\n",
       "      <td>6.586000e+00</td>\n",
       "      <td>3.470800e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.570000e+02</td>\n",
       "      <td>-1.821900e+01</td>\n",
       "      <td>1.138490e+02</td>\n",
       "      <td>2019-01-24 01:18:39.168000</td>\n",
       "      <td>1.312000e+03</td>\n",
       "      <td>1.138600e+01</td>\n",
       "      <td>3.496400e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.920000e+02</td>\n",
       "      <td>1.860100e+01</td>\n",
       "      <td>1.448760e+02</td>\n",
       "      <td>2019-01-31 23:46:43.000017920</td>\n",
       "      <td>5.692400e+03</td>\n",
       "      <td>5.925300e+01</td>\n",
       "      <td>5.304200e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.782093e+01</td>\n",
       "      <td>1.732301e+01</td>\n",
       "      <td>2.865404e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.103844e+02</td>\n",
       "      <td>6.417424e+00</td>\n",
       "      <td>2.131450e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              cycle      latitude     longitude  \\\n",
       "count  1.374813e+06  1.374813e+06  1.374813e+06   \n",
       "mean   1.164047e+02 -3.015141e+01  9.298543e+01   \n",
       "min    1.000000e+00 -6.443400e+01  2.058160e+01   \n",
       "25%    8.000000e+01 -4.244269e+01  7.010800e+01   \n",
       "50%    1.040000e+02 -3.184380e+01  9.536275e+01   \n",
       "75%    1.570000e+02 -1.821900e+01  1.138490e+02   \n",
       "max    4.920000e+02  1.860100e+01  1.448760e+02   \n",
       "std    6.782093e+01  1.732301e+01  2.865404e+01   \n",
       "\n",
       "                                time      pressure   temperature      salinity  \n",
       "count                        1374813  1.089903e+06  1.085000e+06  1.084049e+06  \n",
       "mean   2019-01-16 12:07:59.426492928  8.110287e+02  8.489975e+00  3.459978e+01  \n",
       "min       2019-01-01 00:41:32.640000 -3.171200e+03 -1.709000e+00 -4.010000e+00  \n",
       "25%    2019-01-08 22:04:32.999994368  2.400000e+02  3.298000e+00  3.457620e+01  \n",
       "50%    2019-01-16 11:50:15.999989248  7.240000e+02  6.586000e+00  3.470800e+01  \n",
       "75%       2019-01-24 01:18:39.168000  1.312000e+03  1.138600e+01  3.496400e+01  \n",
       "max    2019-01-31 23:46:43.000017920  5.692400e+03  5.925300e+01  5.304200e+01  \n",
       "std                              NaN  6.103844e+02  6.417424e+00  2.131450e+00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e2d4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float_id            0\n",
       "cycle               0\n",
       "latitude            0\n",
       "longitude           0\n",
       "time                0\n",
       "pressure       284910\n",
       "temperature    289813\n",
       "salinity       290764\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "508bc5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote argo_profiles_metadata.parquet, entries: 1459\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# assume df already loaded from parquet (like you showed)\n",
    "# df columns: float_id, cycle, latitude, longitude, time, pressure, temperature, salinity\n",
    "\n",
    "# clean float_id bytes-looking strings\n",
    "df['float_id'] = df['float_id'].astype(str).str.strip().str.strip(\"b'\").str.strip(\"'\")\n",
    "\n",
    "# Create profile-level summary doc\n",
    "grouped = df.groupby(['float_id', 'cycle'], as_index=False)\n",
    "\n",
    "def make_profile_doc(g):\n",
    "    n_levels = len(g)\n",
    "    lat = float(g['latitude'].iloc[0])\n",
    "    lon = float(g['longitude'].iloc[0])\n",
    "    time = pd.to_datetime(g['time'].iloc[0])\n",
    "    p_min = float(g['pressure'].min()) if 'pressure' in g and g['pressure'].notnull().any() else None\n",
    "    p_max = float(g['pressure'].max()) if 'pressure' in g and g['pressure'].notnull().any() else None\n",
    "    t_min = float(g['temperature'].min()) if 'temperature' in g and g['temperature'].notnull().any() else None\n",
    "    t_max = float(g['temperature'].max()) if 'temperature' in g and g['temperature'].notnull().any() else None\n",
    "    s_min = float(g['salinity'].min()) if 'salinity' in g and g['salinity'].notnull().any() else None\n",
    "    s_max = float(g['salinity'].max()) if 'salinity' in g and g['salinity'].notnull().any() else None\n",
    "    # textual summary: short, factual\n",
    "    summary = (\n",
    "        f\"Float {g['float_id'].iloc[0]} cycle {g['cycle'].iloc[0]} at ({lat:.3f},{lon:.3f}) \"\n",
    "        f\"on {time.date()} with {n_levels} levels; p_range={p_min}-{p_max}, \"\n",
    "        f\"temp={t_min}-{t_max}, sal={s_min}-{s_max}.\"\n",
    "    )\n",
    "    return {\n",
    "        \"float_id\": g['float_id'].iloc[0],\n",
    "        \"cycle\": int(g['cycle'].iloc[0]),\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"time\": pd.to_datetime(time),\n",
    "        \"n_levels\": int(n_levels),\n",
    "        \"p_min\": p_min,\n",
    "        \"p_max\": p_max,\n",
    "        \"t_min\": t_min,\n",
    "        \"t_max\": t_max,\n",
    "        \"s_min\": s_min,\n",
    "        \"s_max\": s_max,\n",
    "        \"summary\": summary\n",
    "    }\n",
    "\n",
    "# Build docs list (this is memory-sensitive for huge data — see scaling below)\n",
    "docs = []\n",
    "for (fid, cyc), grp in df.groupby(['float_id', 'cycle']):\n",
    "    docs.append(make_profile_doc(grp))\n",
    "\n",
    "meta_df = pd.DataFrame(docs)\n",
    "meta_df.to_parquet(\"argo_profiles_metadata.parquet\", index=False)\n",
    "print(\"Wrote argo_profiles_metadata.parquet, entries:\", len(meta_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "894fab81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shubh\\Downloads\\AI ML\\venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Saved FAISS index and metadata. vectors: (1459, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "meta_df = pd.read_parquet(\"argo_profiles_metadata.parquet\")\n",
    "\n",
    "# choose embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # small & fast\n",
    "\n",
    "# prepare texts (summary or combined fields)\n",
    "texts = meta_df['summary'].astype(str).tolist()\n",
    "\n",
    "# compute embeddings in batches\n",
    "batch_size = 256\n",
    "embs = []\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    emb_batch = model.encode(texts[i:i+batch_size], show_progress_bar=False, convert_to_numpy=True)\n",
    "    embs.append(emb_batch)\n",
    "embs = np.vstack(embs).astype('float32')  # FAISS uses float32\n",
    "\n",
    "# Build FAISS index (L2 or inner product; we'll normalize and use inner product)\n",
    "d = embs.shape[1]\n",
    "index = faiss.IndexFlatIP(d)  # inner product\n",
    "# normalize vectors to unit length for cosine similarity with IP\n",
    "faiss.normalize_L2(embs)\n",
    "index.add(embs)\n",
    "\n",
    "# persist index and metadata easily\n",
    "faiss.write_index(index, \"faiss_argo_profiles.index\")\n",
    "meta_df.to_parquet(\"argo_profiles_metadata_with_index.parquet\", index=False)\n",
    "# also save original summaries in a jsonl for quick lookup if desired\n",
    "meta_df.to_json(\"argo_profiles_metadata.jsonl\", orient=\"records\", lines=True)\n",
    "print(\"Saved FAISS index and metadata. vectors:\", embs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bfdf5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'float_id': '3901958 ', 'cycle': 40, 'latitude': -58.18794682152788, 'longitude': 46.62004344838868, 'time': Timestamp('2019-01-30 21:00:30'), 'n_levels': 1208, 'p_min': 0.20000000298023224, 'p_max': 2025.699951171875, 't_min': -0.7950000166893005, 't_max': 2.0309998989105225, 's_min': 33.78900146484375, 's_max': 34.74100112915039, 'summary': 'Float 3901958  cycle 40 at (-58.188,46.620) on 2019-01-30 with 1208 levels; p_range=0.20000000298023224-2025.699951171875, temp=-0.7950000166893005-2.0309998989105225, sal=33.78900146484375-34.74100112915039.', 'score': 0.4799206852912903}\n",
      "{'float_id': '3901649 ', 'cycle': 37, 'latitude': -44.489, 'longitude': 25.753, 'time': Timestamp('2019-01-17 05:54:00'), 'n_levels': 192, 'p_min': 0.0, 'p_max': 1985.0, 't_min': 2.5280001163482666, 't_max': 8.894000053405762, 's_min': 33.81999969482422, 's_max': 34.755001068115234, 'summary': 'Float 3901649  cycle 37 at (-44.489,25.753) on 2019-01-17 with 192 levels; p_range=0.0-1985.0, temp=2.5280001163482666-8.894000053405762, sal=33.81999969482422-34.755001068115234.', 'score': 0.4794118106365204}\n",
      "{'float_id': '2901904 ', 'cycle': 46, 'latitude': -25.0611, 'longitude': 35.6076, 'time': Timestamp('2019-01-25 23:20:39.999996416'), 'n_levels': 43, 'p_min': 4.199999809265137, 'p_max': 950.5999755859375, 't_min': 6.109000205993652, 't_max': 26.83099937438965, 's_min': 34.608001708984375, 's_max': 35.51599884033203, 'summary': 'Float 2901904  cycle 46 at (-25.061,35.608) on 2019-01-25 with 43 levels; p_range=4.199999809265137-950.5999755859375, temp=6.109000205993652-26.83099937438965, sal=34.608001708984375-35.51599884033203.', 'score': 0.4738788604736328}\n",
      "{'float_id': '3901648 ', 'cycle': 38, 'latitude': -42.246, 'longitude': 35.365, 'time': Timestamp('2019-01-28 05:36:00'), 'n_levels': 194, 'p_min': 0.0, 'p_max': 2004.0, 't_min': 2.638000011444092, 't_max': 13.418000221252441, 's_min': 34.28900146484375, 's_max': 34.83700180053711, 'summary': 'Float 3901648  cycle 38 at (-42.246,35.365) on 2019-01-28 with 194 levels; p_range=0.0-2004.0, temp=2.638000011444092-13.418000221252441, sal=34.28900146484375-34.83700180053711.', 'score': 0.47093549370765686}\n",
      "{'float_id': '2901903 ', 'cycle': 45, 'latitude': -25.3704, 'longitude': 33.4861, 'time': Timestamp('2019-01-24 09:08:46.999998208'), 'n_levels': 26, 'p_min': 24.0, 'p_max': 315.70001220703125, 't_min': 12.312999725341797, 't_max': 26.701000213623047, 's_min': 35.09400177001953, 's_max': 35.356998443603516, 'summary': 'Float 2901903  cycle 45 at (-25.370,33.486) on 2019-01-24 with 26 levels; p_range=24.0-315.70001220703125, temp=12.312999725341797-26.701000213623047, sal=35.09400177001953-35.356998443603516.', 'score': 0.4696224629878998}\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "index = faiss.read_index(\"faiss_argo_profiles.index\")\n",
    "meta_df = pd.read_parquet(\"argo_profiles_metadata_with_index.parquet\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def retrieve(query, top_k=5):\n",
    "    q_emb = model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    results = []\n",
    "    for idx, score in zip(I[0], D[0]):\n",
    "        rec = meta_df.iloc[idx].to_dict()\n",
    "        rec['score'] = float(score)\n",
    "        results.append(rec)\n",
    "    return results\n",
    "# example query\n",
    "query = \"Find profiles with temperature above 20 degrees and salinity below 35\"\n",
    "results = retrieve(query, top_k=5)\n",
    "for r in results:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95606a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intent': 'filter_profiles', 'filters': {'lat_range': [-2.0, 2.0], 'lon_range': None, 'start_date': '2023-03-01', 'end_date': '2023-03-31', 'variables': ['salinity'], 'depth_range': None, 'float_ids': None}, 'visualization': 'profile_plot', 'k': 50}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an assistant that extracts a structured query object (JSON) from a user's request.\n",
    "Output only valid JSON matching this schema:\n",
    "{\n",
    "  \"intent\": \"<one of: filter_profiles, compare_regions, nearest_floats, summary>\",\n",
    "  \"filters\": {\n",
    "    \"lat_range\": [min_lat, max_lat] or null,\n",
    "    \"lon_range\": [min_lon, max_lon] or null,\n",
    "    \"start_date\": \"YYYY-MM-DD\" or null,\n",
    "    \"end_date\": \"YYYY-MM-DD\" or null,\n",
    "    \"variables\": [\"temperature\",\"salinity\",\"pressure\",\"oxygen\"] or null,\n",
    "    \"depth_range\": [min_m, max_m] or null,\n",
    "    \"float_ids\": [\"1900975\", ...] or null\n",
    "  },\n",
    "  \"visualization\": \"<one of: profile_plot, map, timeseries, table>\",\n",
    "  \"k\": 50\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "user_query = \"Show salinity profiles near the equator in March 2023\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract JSON string\n",
    "mcp_json_str = response.choices[0].message.content\n",
    "mcp = json.loads(mcp_json_str)\n",
    "print(mcp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7971e0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [float_id, cycle, latitude, longitude, time, n_levels, p_min, p_max, t_min, t_max, s_min, s_max, summary]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# def execute_structured_query(qobj, parquet_path=\"argo_indian_2019_01.parquet\"):\n",
    "def execute_structured_query(qobj, parquet_path=r\"C:\\Users\\shubh\\Downloads\\Classroom\\Hackathons\\SIH 2025\\ARGO\\Test\\argo_profiles_metadata_with_index.parquet\"):\n",
    "    # For efficiency, load only metadata or use pyarrow dataset scanner + filters\n",
    "    # We'll show an in-memory example for clarity: read profile metadata, filter, then read full levels as needed.\n",
    "    meta = pd.read_parquet(\"argo_profiles_metadata_with_index.parquet\")\n",
    "    f = qobj.get(\"filters\", {})\n",
    "    # apply lat/lon filters\n",
    "    if f.get(\"lat_range\"):\n",
    "        minlat, maxlat = f[\"lat_range\"]\n",
    "        meta = meta[(meta.latitude >= minlat) & (meta.latitude <= maxlat)]\n",
    "    if f.get(\"lon_range\"):\n",
    "        minlon, maxlon = f[\"lon_range\"]\n",
    "        meta = meta[(meta.longitude >= minlon) & (meta.longitude <= maxlon)]\n",
    "    if f.get(\"start_date\"):\n",
    "        start = pd.to_datetime(f[\"start_date\"])\n",
    "        meta = meta[meta.time >= start]\n",
    "    if f.get(\"end_date\"):\n",
    "        end = pd.to_datetime(f[\"end_date\"])\n",
    "        meta = meta[meta.time <= end]\n",
    "    if f.get(\"float_ids\"):\n",
    "        meta = meta[meta.float_id.isin(f[\"float_ids\"])]\n",
    "    # limit number of profiles\n",
    "    k = qobj.get(\"k\", 50)\n",
    "    sel = meta.sort_values(\"time\").head(k)\n",
    "    # now read the raw levels for these profiles from the full parquet (df)\n",
    "    full = pd.read_parquet(parquet_path)\n",
    "    merged = full.merge(sel[['float_id','cycle']], on=['float_id','cycle'], how='inner')\n",
    "    # apply depth filter to pressure if provided\n",
    "    if f.get(\"depth_range\") and 'pressure' in merged.columns:\n",
    "        pmin, pmax = f['depth_range']\n",
    "        merged = merged[(merged.pressure >= pmin) & (merged.pressure <= pmax)]\n",
    "    return merged\n",
    "# example usage\n",
    "qobj = {\n",
    "  \"intent\": \"filter_profiles\",\n",
    "  \"filters\": {\n",
    "    \"lat_range\": [-5, 5],\n",
    "    \"lon_range\": [30, 60],\n",
    "    \"start_date\": \"2019-03-01\",\n",
    "    \"end_date\": \"2019-03-31\",\n",
    "    \"variables\": [\"salinity\"],\n",
    "    \"depth_range\": [0, 200],\n",
    "    \"float_ids\": None\n",
    "  },\n",
    "  \"visualization\": \"profile_plot\",\n",
    "  \"k\": 10\n",
    "}\n",
    "df = execute_structured_query(qobj)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0892f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OpenAI (pseudo)\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def llm_get_structured_json(prompt_text):\n",
    "    resp = client.chat.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"system\",\"content\":\"...instructions...\"},\n",
    "                                                           {\"role\":\"user\",\"content\":prompt_text}])\n",
    "    text = resp.choices[0].message.content\n",
    "    # If the model returned extra text, try to extract JSON\n",
    "    import re, json\n",
    "    m = re.search(r'\\{.*\\}$', text, re.S)\n",
    "    raw = m.group(0) if m else text\n",
    "    return json.loads(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a85bb1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Chat' object has no attribute 'create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m qobj, df_result, docs\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# example usage\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mrag_query_and_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mShow salinity profiles near the equator in March 2023\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 16\u001b[0m, in \u001b[0;36mrag_query_and_run\u001b[1;34m(user_query)\u001b[0m\n\u001b[0;32m     10\u001b[0m prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext documents:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser request: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtract a JSON structured query object (see schema). Output only JSON.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 3) call LLM to get JSON (you can use OpenAI or local)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m qobj \u001b[38;5;241m=\u001b[39m \u001b[43mllm_get_structured_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# you must implement this using your LLM provider\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 4) execute structured query\u001b[39;00m\n\u001b[0;32m     18\u001b[0m df_result \u001b[38;5;241m=\u001b[39m execute_structured_query(qobj, parquet_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margo_indian_2019_01.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m, in \u001b[0;36mllm_get_structured_json\u001b[1;34m(prompt_text)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mllm_get_structured_json\u001b[39m(prompt_text):\n\u001b[1;32m----> 6\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m, messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...instructions...\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      7\u001b[0m                                                            {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:prompt_text}])\n\u001b[0;32m      8\u001b[0m     text \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# If the model returned extra text, try to extract JSON\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Chat' object has no attribute 'create'"
     ]
    }
   ],
   "source": [
    "# assume retrieve() defined earlier, and you have an LLM client function llm_get_structured_json(query_text)\n",
    "# llm_get_structured_json calls your LLM and returns parsed json\n",
    "\n",
    "def rag_query_and_run(user_query):\n",
    "    # 1) retrieval to give context\n",
    "    docs = retrieve(user_query, top_k=6)\n",
    "    # combine top docs as context for LLM (short)\n",
    "    context = \"\\n\".join([d['summary'] for d in docs])\n",
    "    # 2) build prompt with context + user query\n",
    "    prompt = (\n",
    "        f\"Context documents:\\n{context}\\n\\n\"\n",
    "        f\"User request: {user_query}\\n\\n\"\n",
    "        \"Extract a JSON structured query object (see schema). Output only JSON.\"\n",
    "    )\n",
    "    # 3) call LLM to get JSON (you can use OpenAI or local)\n",
    "    qobj = llm_get_structured_json(prompt)  # you must implement this using your LLM provider\n",
    "    # 4) execute structured query\n",
    "    df_result = execute_structured_query(qobj, parquet_path=\"argo_indian_2019_01.parquet\")\n",
    "    return qobj, df_result, docs\n",
    "\n",
    "# example usage\n",
    "rag_query_and_run(\"Show salinity profiles near the equator in March 2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b75cea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 01:53:39.002 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 01:53:39.246 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\shubh\\Downloads\\AI ML\\venv\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-09-23 01:53:39.247 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 01:53:39.247 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 01:53:39.248 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 01:53:39.249 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 01:53:39.250 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 01:53:39.251 Session state does not function when running a script without `streamlit run`\n",
      "2025-09-23 01:53:39.252 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 01:53:39.253 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 01:53:39.254 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 01:53:39.254 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 01:53:39.256 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 01:53:39.257 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 01:53:39.258 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import plotly.express as px\n",
    "\n",
    "st.title(\"FloatChat — ARGO RAG Demo (Parquet-only)\")\n",
    "\n",
    "user_q = st.text_input(\"Ask about ARGO data (e.g., 'Show salinity near equator March 2019')\")\n",
    "\n",
    "if st.button(\"Run\"):\n",
    "    with st.spinner(\"Running RAG...\"):\n",
    "        qobj, df_res, docs = rag_query_and_run(user_q)\n",
    "    st.subheader(\"Structured Query\")\n",
    "    st.json(qobj)\n",
    "    if df_res.empty:\n",
    "        st.info(\"No matching profiles found.\")\n",
    "    else:\n",
    "        st.subheader(\"Sample rows\")\n",
    "        st.dataframe(df_res.head(200))\n",
    "\n",
    "        # profile plot example: salinity vs pressure per profile\n",
    "        if 'salinity' in df_res.columns:\n",
    "            fig = px.line(df_res, x='salinity', y='pressure', color='float_id', line_group=['float_id','cycle'],\n",
    "                          labels={'pressure':'Pressure (dbar)', 'salinity':'Salinity PSU'},\n",
    "                          title='Salinity profiles (pressure increasing downward)')\n",
    "            fig.update_yaxes(autorange='reversed')  # pressure downwards\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "        # map of profile locations\n",
    "        map_df = df_res.groupby(['float_id','cycle','latitude','longitude']).size().reset_index()\n",
    "        st.map(map_df.rename(columns={'latitude':'lat','longitude':'lon'}).loc[:,['lat','lon']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f04de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
